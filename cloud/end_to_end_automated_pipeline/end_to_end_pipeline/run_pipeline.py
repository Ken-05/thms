# run_pipeline.py
import os
from google.cloud import aiplatform
import yaml

# Trigger and initiate a single execution of the Kubeflow 
# Pipeline on Google Cloud's Vertex AI pipelines using
# the Vertex AI SDK.

# Uses the google-cloud-aiplatform SDK to submit the
# compiled pipeline definition.

# This code takes the compiled pipeline JSON file, 
# (the blueprint generated by end_to_end_automated_pipeline.py) 
# as an input.

    
# This script is for manually triggering the pipeline.
# For automated scheduling, the compiled pipeline JSON will be uploaded to GCS
# and a recurring run will be configured directly in the Vertex AI Console, 
# Vertex AI Schedule.
# The equivalent of this script would then be performed on a recurring schedule 
# using Vertex AI schedule at 3AM everyday.


# --- Function to load cloud_settings.yaml ---
def load_cloud_settings():
    """
    Loads configuration settings from the cloud_settings.yaml file.
    Assumes cloud_settings.yaml is in the 'config' directory at the project root.
    """
    BASE_DIR = os.path.abspath(os.path.dirname(__file__))
    CONFIG_DIR = os.path.join(BASE_DIR, "..", "config") # Go up one level to project_root, then into 'config'
    CLOUD_SETTINGS_FILE_PATH = os.path.join(CONFIG_DIR, "cloud_settings.yaml")

    try:
        with open(CLOUD_SETTINGS_FILE_PATH, 'r') as f:
            settings = yaml.safe_load(f)
        print(f"[RunPipeline:Settings] Loaded cloud settings from {CLOUD_SETTINGS_FILE_PATH}")
        return settings
    except FileNotFoundError:
        print(f"[RunPipeline:Settings] ERROR: cloud_settings.yaml not found at {CLOUD_SETTINGS_FILE_PATH}. Exiting.")
        exit(1) # Critical error, cannot proceed without cloud settings
    except yaml.YAMLError as e:
        print(f"[RunPipeline:Settings] ERROR: Could not decode YAML from {CLOUD_SETTINGS_FILE_PATH}. Check file format. Error: {e}")
        exit(1) # Critical error

# Load cloud settings once when the script is run
CLOUD_SETTINGS = load_cloud_settings()

# --- Configuration (loaded from CLOUD_SETTINGS) ---
# Defines the Google Cloud Project ID.
PROJECT_ID = CLOUD_SETTINGS['gcp_project_id']
# Defines the Google Cloud Region.
REGION = CLOUD_SETTINGS['gcp_region']

# Specifies the path to the compiled Kubeflow Pipeline JSON file.
BASE_DIR = os.path.abspath(os.path.dirname(__file__))
COMPILED_PIPELINE_OUTPUT_FILE = CLOUD_SETTINGS['vertex_ai_pipelines']['compiled_pipeline_output_file']
PIPELINE_JSON_PATH = os.path.join(
    BASE_DIR,
    COMPILED_PIPELINE_OUTPUT_FILE # The name of the compiled pipeline JSON
)
PIPELINE_JSON_PATH = os.path.normpath(PIPELINE_JSON_PATH) # Normalize path

# Defines the Google Cloud Storage (GCS) bucket path for pipeline run artifacts.
# This value aligns with the 'pipeline_root' specified in the KFP pipeline decorator.
PIPELINE_ROOT = f"gs://{CLOUD_SETTINGS['gcs']['pipeline_artifacts_bucket']}/"

# Sets a display name for the pipeline job in the Vertex AI console.
DISPLAY_NAME = CLOUD_SETTINGS['vertex_ai_pipelines']['pipeline_name']


def run_pipeline():
    """
    Initializes the Vertex AI SDK and submits the compiled Kubeflow Pipeline job.
    """
    # Initializes the AI Platform client with the specified project, location, and staging bucket.
    # The staging_bucket is used for temporary files during pipeline execution.
    aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=PIPELINE_ROOT,)
   
    # Creates a PipelineJob instance using the defined parameters.
    job = aiplatform.PipelineJob(
        display_name=DISPLAY_NAME,
        template_path=PIPELINE_JSON_PATH,
        pipeline_root=PIPELINE_ROOT,
        enable_caching=True,
    )
    
    # Executes the pipeline job.
    print(f"[RunPipeline] Submitting pipeline job '{DISPLAY_NAME}'...")
    job.run(sync=True)  # sync=True waits for the job to pipeline job to finish
    print(f"[RunPipeline] Pipeline job '{DISPLAY_NAME}' completed.")

if __name__ == "__main__":
    print(f"Starting pipeline run for project: {PROJECT_ID}, region: {REGION}")
    print(f"Pipeline JSON path: {PIPELINE_JSON_PATH}")
    print(f"Pipeline Root: {PIPELINE_ROOT}")
    run_pipeline()
    print("Pipeline run initiated and completed.")
    
