import os
import argparse
import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow.keras import layers, models, callbacks
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
import keras_tuner as kt
import shap
#from joblib import dump
from preprocess import preprocess
#import matplotlib.pyplot as plt

GCS_TRAIN_DATA_PATH = "gs://tractor-health-monitoring-bucket/data/train_data.csv"
GCS_OUTPUT_DIR = "gs://tractor-health-monitoring-bucket/models"


# -----------------------------
# Build classifier model with tunable hyperparameters
# -----------------------------
def build_classifier_model(hp):
    model = models.Sequential()
    model.add(layers.Input(shape=(input_shape,)))
    
    # Tune number of layers
    for i in range(hp.Int('num_layers', 1, 3)):
        model.add(layers.Dense(units=hp.Int(f'units_{i}', 32, 256, step=32), activation='relu'))
        model.add(layers.Dropout(rate=hp.Float(f'dropout_{i}', 0.0, 0.5, step=0.1)))
    
    model.add(layers.Dense(num_classes, activation='softmax'))
    
    model.compile(
        optimizer=tf.keras.optimizers.Adam(hp.Float('learning_rate', 1e-4, 1e-2, sampling='log')),
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy']
    )
    return model

# -----------------------------
# Run hyperparameter tuner
# -----------------------------
def run_tuner(X_train, Y_train):
    tuner = kt.RandomSearch(
        build_classifier_model,
        objective='val_accuracy',
        max_trials=10,
        executions_per_trial=1,
        overwrite=True,
        directory='tuner_logs',
        project_name='classifier_tuning'
    )
    
    tuner.search_space_summary()
    
    tuner.search(
        X_train, Y_train,
        epochs=30,
        validation_split=0.2,
        callbacks=[callbacks.EarlyStopping(monitor="val_loss", patience=5)],
        verbose=1
    )
    
    tuner.results_summary()
    best_model = tuner.get_best_models(num_models=1)[0]
    return best_model

# -----------------------------
# Explain predictions with SHAP
# -----------------------------
def explain_model(model, X_sample):
    explainer = shap.KernelExplainer(model.predict, shap.sample(X_sample, 100))
    shap_values = explainer.shap_values(X_sample)
    
    shap.summary_plot(shap_values, X_sample)

# -----------------------------
# Main training and evaluation function
# -----------------------------
def train_and_evaluate(X_Y_train, X_Y_val, model, output_dir):
    es = callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=1)

    checkpoint_path = os.path.join(output_dir, "best_classifier_model.h5")
    checkpoint_cb = callbacks.ModelCheckpoint(
        filepath=checkpoint_path,
        monitor="val_loss",
        save_best_only=True,
        verbose=1
    )
    
    # Train best model further or fine-tune if needed
    history = model.fit(
        X_Y_train[0], X_Y_train[1],
        epochs=50,
        batch_size=32,
        validation_data=(X_Y_val[0], X_Y_val[1]),
        callbacks=[es, checkpoint_cb],
        verbose=1
    )
    
    # Evaluate model
    loss, acc = model.evaluate(X_Y_val[0], X_Y_val[1])
    print(f"[Evaluation] Test Accuracy: {acc:.4f}")
    
    # Explain model predictions on test set sample
    explain_model(model, X_Y_val[0].sample(200))
    
    # Save model
    final_model_path = os.path.join(output_dir, "classifier_model.h5")
    model.save(final_model_path)
    print(f"Saved model to {final_model_path}")
    
    # Convert to TFLite and save
    convert_and_save_tflite(model, output_dir)
    
        
    # model.save(os.path.join(MODEL_DIR, "classifier_model.h5"))
    # print(f"Model saved to {os.path.join(MODEL_DIR, 'classifier_model.h5')}")
    
    # Convert to TFLite and save
    #convert_and_save_tflite(model)


# -----------------------------
# Convert to TfLite
# -----------------------------
def convert_and_save_tflite(model, output_dir):
    # Convert the model to TFLite format
    converter = tf.lite.TFLiteConverter.from_keras_model(model)
    tflite_model = converter.convert()

    # Save tflite model
    tflite_model_path = os.path.join(output_dir, "model_classifier.tflite")
    with tf.io.gfile.GFile(tflite_model_path, "wb") as f:
        f.write(tflite_model)
    print(f"TFLite model saved to {tflite_model_path}")
    
    

# -----------------------------
# Main
# -----------------------------
def main():
    print("[Training] Starting preprocessing...")
    df, scaler = preprocess(GCS_TRAIN_DATA_PATH)

    print("[Training] Preparing classification data...")
    X = df.drop(columns=["timestamp", "fault_label"])
    y = df["fault_label"]

    global input_shape, num_classes
    input_shape = X.shape[1]
    num_classes = len(np.unique(y))

    label_encoder = LabelEncoder()
    y_encoded = label_encoder.fit_transform(y)

    X_train, X_val, y_train, y_val = train_test_split(X, y_encoded, test_size=0.2, random_state=42)

    print("[Training] Running hyperparameter tuning...")
    best_model = run_tuner(X_train, y_train)

    print("[Training] Training final model...")
    train_and_evaluate((X_train, y_train), (X_val, y_val), best_model, GCS_OUTPUT_DIR)

    # scaler_path = os.path.join(GCS_OUTPUT_DIR, "scaler.joblib")
    # with tf.io.gfile.GFile(scaler_path, "wb") as f:
        # dump(scaler, f)
    # print(f"Scaler saved to {scaler_path}")

if __name__ == "__main__":
    main()